{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/prashannagc/Documents/neural network/multimodal_sentiment_analysis/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import ssl\n",
        "import os\n",
        "import certifi\n",
        "\n",
        "# Fix SSL certificate verification errors\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "os.environ['CURL_CA_BUNDLE'] = ''\n",
        "os.environ['REQUESTS_CA_BUNDLE'] = ''\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../data/amazon_fashion_reviews.csv\")\n",
        "\n",
        "# Map ratings to sentiment classes\n",
        "def map_sentiment(rating):\n",
        "    if rating <= 2:\n",
        "        return 0  # Negative\n",
        "    elif rating == 3:\n",
        "        return 1  # Neutral\n",
        "    else:\n",
        "        return 2  # Positive\n",
        "\n",
        "df['sentiment'] = df['reviews.rating'].apply(map_sentiment)\n",
        "\n",
        "# Split dataset\n",
        "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['sentiment'])\n",
        "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42, stratify=test_df['sentiment'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "MAX_LEN = 128  # truncate/pad to 128 tokens\n",
        "\n",
        "def tokenize_text(text):\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LEN,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return encoding['input_ids'].squeeze(), encoding['attention_mask'].squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, df, transform=None, tokenizer=None, max_length=128):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # ----- Text -----\n",
        "        text = str(row['reviews.text'])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze(0)  # remove batch dim\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "\n",
        "        # ----- Image -----\n",
        "        img = None\n",
        "        url = row['reviews.sourceURLs']\n",
        "        if isinstance(url, str) and url.strip() != \"\":\n",
        "            try:\n",
        "                response = requests.get(url, timeout=5)\n",
        "                img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "            except:\n",
        "                img = torch.zeros(3, 224, 224)  # fallback blank image\n",
        "        else:\n",
        "            img = torch.zeros(3, 224, 224)\n",
        "\n",
        "     \n",
        "        label = torch.tensor(row['sentiment'], dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'image': img,\n",
        "            'label': label\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = MultimodalDataset(train_df, transform=image_transforms, tokenizer=tokenizer)\n",
        "val_dataset   = MultimodalDataset(val_df, transform=image_transforms, tokenizer=tokenizer)\n",
        "test_dataset  = MultimodalDataset(test_df, transform=image_transforms, tokenizer=tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class MultimodalSentimentModel(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super().__init__()\n",
        "        # Text Model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.text_fc = nn.Linear(768, 256)\n",
        "\n",
        "        # Image Model\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 256)\n",
        "\n",
        "        # Fusion\n",
        "        self.fc1 = nn.Linear(256+256, 128)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images):\n",
        "        # Text forward\n",
        "        text_emb = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "        text_feat = self.text_fc(text_emb)\n",
        "\n",
        "        # Image forward\n",
        "        img_feat = self.resnet(images)\n",
        "\n",
        "        # Concatenate\n",
        "        combined = torch.cat((text_feat, img_feat), dim=1)\n",
        "        x = self.fc1(combined)\n",
        "        x = self.dropout(x)\n",
        "        out = self.out(x)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/prashannagc/Documents/neural network/multimodal_sentiment_analysis/venv/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/prashannagc/Documents/neural network/multimodal_sentiment_analysis/venv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultimodalSentimentModel().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "EPOCHS = 3  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: Train Loss=0.4814, Train Acc=0.8675 | Val Loss=1.5684, Val Acc=0.8750\n"
          ]
        }
      ],
      "source": [
        "train_loss_list, val_loss_list = [], []\n",
        "train_acc_list, val_acc_list = [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / len(train_dataset)\n",
        "    train_loss_list.append(train_loss)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    val_loss_list.append(val_loss/len(val_loader))\n",
        "    val_acc_list.append(val_correct/len(val_dataset))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}: \"\n",
        "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
        "          f\"Val Loss={val_loss_list[-1]:.4f}, Val Acc={val_acc_list[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "torch.save(model.state_dict(), \"../models/multimodal_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_loss_list, label=\"Train Loss\")\n",
        "plt.plot(val_loss_list, label=\"Val Loss\")\n",
        "plt.title(\"Loss vs Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_acc_list, label=\"Train Accuracy\")\n",
        "plt.plot(val_acc_list, label=\"Val Accuracy\")\n",
        "plt.title(\"Accuracy vs Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for input_ids, attention_mask, images, labels in test_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        preds = outputs.argmax(1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Negative\",\"Neutral\",\"Positive\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for i in range(5):\n",
        "    print(\"Review Text:\", test_df.iloc[i]['reviews.text'])\n",
        "    print(\"True Sentiment:\", test_df.iloc[i]['sentiment'])\n",
        "    print(\"Predicted Sentiment:\", y_pred[i])\n",
        "    print(\"-\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "\n",
        "print(\"Model Performance on Test Set:\")\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall   : {recall:.4f}\")\n",
        "print(f\"F1-score : {f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
